
=== Inputs ===
GPU: 0
Sample Size: 120
Model root: ./trained_models/SQL/
Models: ['M123']
Negated Models: []
Config Type: <class 'modeling.secureLLM.slora.SloraLogitConfig'>
Weights: None
val_str: schema_1_val_gpt:120,schema_2_val_gpt:120,schema_3_val_gpt:120,schema_union12_val_gpt:120,schema_union13_val_gpt:120,schema_union23_val_gpt:120,schema_union123_val_gpt:120

=== Starting Script ===
Hugging Face Llama Model successfully loaded
Slora Configuration loaded and initialized
Adapter checkpoints loaded and linked
Starting validation test
Using KV Cache
['schema_1_val_gpt', 'schema_2_val_gpt', 'schema_3_val_gpt', 'schema_union123_val_gpt', 'schema_union12_val_gpt', 'schema_union13_val_gpt', 'schema_union23_val_gpt', 'no_acc', 'unk']
{'schema_1_val_gpt': {'v': [95, 120, 11], 'to_print': '79.2% (11)', 'to_save': None}, 'schema_2_val_gpt': {'v': [68, 120, 20], 'to_print': '56.7% (20)', 'to_save': None}, 'schema_3_val_gpt': {'v': [62, 120, 38], 'to_print': '51.7% (38)', 'to_save': None}, 'schema_union123_val_gpt': {'v': [0, 120, 119], 'to_print': '0.0% (119)', 'to_save': None}, 'schema_union12_val_gpt': {'v': [0, 120, 120], 'to_print': '0.0% (120)', 'to_save': None}, 'schema_union13_val_gpt': {'v': [4, 120, 108], 'to_print': '3.3% (108)', 'to_save': None}, 'schema_union23_val_gpt': {'v': [3, 120, 110], 'to_print': '2.5% (110)', 'to_save': None}, 'no_acc': {'count': 0, 'to_save': None}, 'unk': {'to_save': None}}
Generation did not finish for (2/840) samples. Consider increasing gen_max_len, currently 480

=== Inputs ===
GPU: 0
Sample Size: 120
val_str: schema_1_val_gpt:120,schema_2_val_gpt:120,schema_3_val_gpt:120,schema_union12_val_gpt:120,schema_union13_val_gpt:120,schema_union23_val_gpt:120,schema_union123_val_gpt:120
Model root: ./trained_models/SQL/
Models: ['M123']
Negated Models: []
Config Type: <class 'modeling.secureLLM.slora.SloraLogitConfig'>
Weights: None

=== Results ===
schema_1_val_gpt,79.16666666666667,0.5833333333333334,0.07066348003848004
schema_2_val_gpt,56.666666666666664,1.1416666666666666,0.20279431216931212
schema_3_val_gpt,51.666666666666664,1.65,0.19513051412238708
schema_union123_val_gpt,0.0,3.066666666666667,0.4355555555555556
schema_union12_val_gpt,0.0,5.683333333333334,0.4342365131500195
schema_union13_val_gpt,3.3333333333333335,1.6666666666666667,0.30458333333333343
schema_union23_val_gpt,2.5,1.5916666666666666,0.34444444444444444
