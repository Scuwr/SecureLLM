
=== Inputs ===
GPU: 0
Sample Size: 120
Model root: ./trained_models/SQL/
Models: ['M1', 'M2', 'M3']
Negated Models: []
Config Type: <class 'modeling.secureLLM.slora.SloraLogitConfig'>
Weights: None
val_str: schema_1_val_gpt:120,schema_2_val_gpt:120,schema_3_val_gpt:120,schema_union12_val_gpt:120,schema_union13_val_gpt:120,schema_union23_val_gpt:120,schema_union123_val_gpt:120

=== Starting Script ===
Hugging Face Llama Model successfully loaded
Slora Configuration loaded and initialized
Adapter checkpoints loaded and linked
Starting validation test
Using KV Cache
['schema_1_val_gpt', 'schema_2_val_gpt', 'schema_3_val_gpt', 'schema_union123_val_gpt', 'schema_union12_val_gpt', 'schema_union13_val_gpt', 'schema_union23_val_gpt', 'no_acc', 'unk']
{'schema_1_val_gpt': {'v': [25, 120, 93], 'to_print': '20.8% (93)', 'to_save': None}, 'schema_2_val_gpt': {'v': [46, 120, 65], 'to_print': '38.3% (65)', 'to_save': None}, 'schema_3_val_gpt': {'v': [32, 120, 76], 'to_print': '26.7% (76)', 'to_save': None}, 'schema_union123_val_gpt': {'v': [1, 120, 119], 'to_print': '0.8% (119)', 'to_save': None}, 'schema_union12_val_gpt': {'v': [0, 120, 120], 'to_print': '0.0% (120)', 'to_save': None}, 'schema_union13_val_gpt': {'v': [3, 120, 115], 'to_print': '2.5% (115)', 'to_save': None}, 'schema_union23_val_gpt': {'v': [2, 120, 112], 'to_print': '1.7% (112)', 'to_save': None}, 'no_acc': {'count': 0, 'to_save': None}, 'unk': {'to_save': None}}
Generation did not finish for (1/840) samples. Consider increasing gen_max_len, currently 480

=== Inputs ===
GPU: 0
Sample Size: 120
val_str: schema_1_val_gpt:120,schema_2_val_gpt:120,schema_3_val_gpt:120,schema_union12_val_gpt:120,schema_union13_val_gpt:120,schema_union23_val_gpt:120,schema_union123_val_gpt:120
Model root: ./trained_models/SQL/
Models: ['M1', 'M2', 'M3']
Negated Models: []
Config Type: <class 'modeling.secureLLM.slora.SloraLogitConfig'>
Weights: None

=== Results ===
schema_1_val_gpt,20.833333333333332,3.9833333333333334,0.5727356671106669
schema_2_val_gpt,38.333333333333336,1.775,0.3073006854256854
schema_3_val_gpt,26.666666666666668,3.6416666666666666,0.4371161811757785
schema_union123_val_gpt,0.8333333333333334,3.6666666666666665,0.5851388888888889
schema_union12_val_gpt,0.0,7.533333333333333,0.6237554446900422
schema_union13_val_gpt,2.5,2.191666666666667,0.4149305555555556
schema_union23_val_gpt,1.6666666666666667,1.7333333333333334,0.38402777777777775
